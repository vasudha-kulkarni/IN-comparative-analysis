{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code to differentiate the differences statistically.\n",
    "\n",
    "13/11/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\4th Year\\\\Semester 7\\\\BI4313 Sem Project\\\\IN-comparative-analysis\\\\IN-comparative-analysis\\\\Quant\\\\Updated_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions for ANOVA - \n",
    "1. Measurements are made randomly from a population\n",
    "2. Variable is normally distributed in each of the populations\n",
    "3. Variance is the same in all k populations\n",
    "\n",
    "BUT: ANOVA is robust to departures from the assumption of equal variance (Ch 15, Whitlock and Schuster), but only if samples are all large, about the same size and no more than 10-fold difference among variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality test for ANOVA - scipy.stats.shapiro(x)\n",
    "\n",
    "Shapiro Wilk test is suited for smaller sample size < 50. If something is not normally distributed, then either the data has to be transformed, or we've to use a test that doesn't assume normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of this species are NOT normally distrbuted: BCC\n",
      "Variance = 0.03938128543132299\n",
      "The values of this species are NOT normally distrbuted: BF\n",
      "Variance = 0.02642834467120182\n",
      "The values of this species are NOT normally distrbuted: JF\n",
      "Variance = 0.08310828821324451\n",
      "The values of this species are NOT normally distrbuted: ZF\n",
      "Variance = 0.040044285982360706\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"01. Song_quantifiers.csv\")\n",
    "#data = pd.read_csv(\"02. Syll_quantifiers.csv\")\n",
    "data = pd.read_csv(\"07. Start-Self Transition Prob.csv\")\n",
    "#data = pd.read_csv(\"08. IN Start-Self Transition Prob.csv\")\n",
    "\n",
    "#Subset data and keep the parameter\n",
    "df1 = data[['Bird name', 'Species', 'Self-transition prob']] #c1\n",
    "\n",
    "species = ['BCC', 'BF', 'JF', 'ZF']\n",
    "\n",
    "for i in range(len(species)):\n",
    "    df2 = df1.loc[df1['Species'] == species[i]]\n",
    "    df3 = df2['Self-transition prob'] #c2\n",
    "    k2, p = stats.shapiro(df3)\n",
    "    alpha = 0.05\n",
    "    if p < alpha:\n",
    "        print(f'The values of this species are NOT normally distrbuted: {species[i]}')\n",
    "    else:\n",
    "        print(f'Normally distrbuted: {species[i]}')\n",
    "    std_dev = np.std(df3)\n",
    "    var = std_dev**2\n",
    "    print(f'Variance = {var}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = df1.loc[df1['Species'] == 'BCC']\n",
    "a2 = a1['Self-transition prob'].to_list()\n",
    "\n",
    "b1 = df1.loc[df1['Species'] == 'BF']\n",
    "b2 = b1['Self-transition prob'].to_list()\n",
    "\n",
    "c1 = df1.loc[df1['Species'] == 'JF']\n",
    "c2 = c1['Self-transition prob'].to_list()\n",
    "\n",
    "d1 = df1.loc[df1['Species'] == 'ZF']\n",
    "d2 = d1['Self-transition prob'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.f_oneway(a2, b2, c2, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ANOVA p-value is significant, it tells us one group is significantly different than the others, but it doesn't really tell us which one it is. So, we do a post hoc Tukey-Kramer test (Tukey for equal sample size, T-K for unequal), which allows us to make pairwise comparisons.\n",
    "\n",
    "#90% sure that 'pairwise_tukeyhsd' is the same as Tukey-Kramer test, it's not explicitly mentioned in the documentation.\n",
    "\n",
    "Alaternative - use Scheffe's method for a post hoc test - applicable in all conditions, but more conservative than Tukey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "=====================================================\n",
      "group1 group2 meandiff p-adj   lower    upper  reject\n",
      "-----------------------------------------------------\n",
      "   BCC     BF    8.648  0.001   5.7075 11.5885   True\n",
      "   BCC     JF   3.5754 0.0134   0.6348  6.5159   True\n",
      "   BCC     ZF    0.843  0.781   -1.736   3.422  False\n",
      "    BF     JF  -5.0727 0.0015  -8.3349 -1.8104   True\n",
      "    BF     ZF   -7.805  0.001 -10.7455 -4.8645   True\n",
      "    JF     ZF  -2.7323 0.0749  -5.6729  0.2082  False\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'values' : a2 + b2 + c2 + d2,\n",
    "    'label' : np.repeat(['BCC', 'BF', 'JF', 'ZF'], repeats=[len(a2), len(b2), len(c2), len(d2)])\n",
    "    })\n",
    "\n",
    "#Not valid right now, because variance is too much and sample size is unequal\n",
    "tuk = pairwise_tukeyhsd(endog=df['values'], groups=df['label'], alpha=0.05)\n",
    "print(tuk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kruskal-Wallis test - non-parametric alternative for ANOVA (similar to Mann Whitney U test, but for multiple groups). But this test is less powerful for small sample sizes. Assumptions - \n",
    "1. All group saples were drawn randomly\n",
    "2. Distribution of variable must have the same shape in every population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=17.718210233145633, pvalue=0.0005028042641747661)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kruskal(a2, b2, c2, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunn test - this is the non-parametric post-hoc test used to do pairwise comparison after Kruskal-Wallis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          BCC        BF        JF        ZF\n",
      "BCC  1.000000  0.526943  0.000031  0.134983\n",
      "BF   0.526943  1.000000  0.004837  0.576338\n",
      "JF   0.000031  0.004837  1.000000  0.008733\n",
      "ZF   0.134983  0.576338  0.008733  1.000000\n",
      "       BCC     BF     JF     ZF\n",
      "BCC  False  False   True  False\n",
      "BF   False  False   True  False\n",
      "JF    True   True  False   True\n",
      "ZF   False  False   True  False\n"
     ]
    }
   ],
   "source": [
    "data = [a2, b2, c2, d2]\n",
    "col = ['BCC', 'BF', 'JF', 'ZF']\n",
    "\n",
    "dunn = sp.posthoc_dunn(data)\n",
    "# Doesn't give significant result, probably because of extra corrections?\n",
    "#dunn = sp.posthoc_dunn(data, p_adjust='bonferroni') \n",
    "\n",
    "dunn.columns = col\n",
    "dunn.index = col\n",
    "\n",
    "print(dunn)\n",
    "print(dunn < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signed Rank test for Amplitude, Frequency, Duration and Interval of first and fifth syllable\n",
    "\n",
    "Wilcoxon test assumes even distribution around the median i.e., no skew which is similar to the normality assumption, restricting its use.\n",
    "If normality condition is not met, simply Sign Test can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wilcoxon signed-rank test tests the null hypothesis that two related paired samples come from the same distribution. In particular, it tests whether the distribution of the differences x - y is symmetric about zero. It is a non-parametric version of the paired T-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normally distrbuted #1: BCC\n",
      "Normally distrbuted #5: BCC\n",
      "Variance #1 = 8441.236523775713 and Variance #5 = 1233.7611071954182\n",
      "Normally distrbuted #1: BF\n",
      "Normally distrbuted #5: BF\n",
      "Variance #1 = 20948.311494024667 and Variance #5 = 732.9232362166167\n",
      "Normally distrbuted #1: JF\n",
      "Normally distrbuted #5: JF\n",
      "Variance #1 = 18000.229682536414 and Variance #5 = 9645.689852715213\n",
      "Normally distrbuted #1: ZF\n",
      "Normally distrbuted #5: ZF\n",
      "Variance #1 = 13470.565616813237 and Variance #5 = 291.04630716036627\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"06. Avg internote interval values.csv\") #c1\n",
    "\n",
    "df1 = data[['Bird name', 'Species', '#1', '#5']]\n",
    "\n",
    "species = ['BCC', 'BF', 'JF', 'ZF']\n",
    "\n",
    "for i in range(len(species)):\n",
    "    df2 = df1.loc[df1['Species'] == species[i]]\n",
    "    df3 = df2['#1']\n",
    "    k1, p1 = stats.shapiro(df3)\n",
    "    df4 = df2['#5']\n",
    "    k2, p2 = stats.shapiro(df4)\n",
    "    alpha = 0.05\n",
    "    if p1 < alpha:\n",
    "        print(f'The values of this species #1 are NOT normally distrbuted: {species[i]}')\n",
    "    else:\n",
    "        print(f'Normally distrbuted #1: {species[i]}')\n",
    "    if p2 < alpha:\n",
    "        print(f'The values of this species #5 are NOT normally distrbuted: {species[i]}')\n",
    "    else:\n",
    "        print(f'Normally distrbuted #5: {species[i]}')\n",
    "    std_dev1 = np.std(df3)\n",
    "    std_dev2 = np.std(df4)\n",
    "    var1 = std_dev1**2\n",
    "    var2 = std_dev2**2\n",
    "    print(f'Variance #1 = {var1} and Variance #5 = {var2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test p-value for BCC is 0.0078125\n",
      "Wilcoxon test p-value for BF is 0.0625\n",
      "Wilcoxon test p-value for JF is 0.0625\n",
      "Wilcoxon test p-value for ZF is 0.0078125\n"
     ]
    }
   ],
   "source": [
    "species = ['BCC', 'BF', 'JF', 'ZF']\n",
    "\n",
    "for i in range(len(species)):\n",
    "    df2 = df1.loc[df1['Species'] == species[i]]\n",
    "    v1 = df2['#1']\n",
    "    v2 = df2['#5']\n",
    "    #'alternative' states the alternative hypothesis - \n",
    "    #for amplitude and frequency, it's 'greater', but interval it's 'less'\n",
    "    s, p = stats.wilcoxon(v2, v1, alternative='two-sided')\n",
    "    print(f'Wilcoxon test p-value for {species[i]} is {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, CI):\n",
    "    \n",
    "    alpha = 1-CI/100\n",
    "    \n",
    "    if len(df.index)<2: df = df.rename(columns = {\"p-unc\" : \"pval\"})    #the pval column  has different names based on test and numerosity\n",
    "    else: df = df.rename(columns = {\"p-corr\" : \"pval\"})\n",
    "    \n",
    "    df.rename({'A': 'group1', 'B': 'group2', \"pval\":\"p-adj\"}, axis=\"columns\", inplace=True)\n",
    "    \n",
    "    '''\n",
    "    Creates a compact letter display. This creates a dataframe consisting of\n",
    "    2 columns, a column containing the treatment groups and a column containing\n",
    "    the letters that have been assigned to the treatment groups. These letters\n",
    "    are part of what's called the compact letter display. Treatment groups that\n",
    "    share at least 1 letter are similar to each other, while treatment groups\n",
    "    that don't share any letters are significantly different from each other.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Pandas dataframe containing raw Tukey test results from statsmodels.\n",
    "    alpha : float\n",
    "        The alpha value. The default is 0.05.\n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe representing the compact letter display, created from the Tukey\n",
    "    test results.\n",
    "    '''\n",
    "    df[\"p-adj\"] = df[\"p-adj\"].astype(float)\n",
    "\n",
    "    # Creating a list of the different treatment groups from Tukey's\n",
    "    group1 = set(df.group1.tolist())  # Dropping duplicates by creating a set\n",
    "    group2 = set(df.group2.tolist())  # Dropping duplicates by creating a set\n",
    "    groupSet = group1 | group2  # Set operation that creates a union of 2 sets\n",
    "    groups = sorted(list(groupSet))\n",
    "\n",
    "    # Creating lists of letters that will be assigned to treatment groups\n",
    "    letters = list(string.ascii_lowercase+string.digits)[:len(groups)]\n",
    "    cldgroups = letters\n",
    "\n",
    "    # the following algoritm is a simplification of the classical cld,\n",
    "\n",
    "    cld = pd.DataFrame(list(zip(groups, letters, cldgroups)))\n",
    "    cld[3]=\"\"\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        if df[\"p-adj\"][row[0]] > (alpha):\n",
    "            cld.iat[groups.index(df[\"group1\"][row[0]]), 2] += cld.iat[groups.index(df[\"group2\"][row[0]]), 1]\n",
    "            cld.iat[groups.index(df[\"group2\"][row[0]]), 2] += cld.iat[groups.index(df[\"group1\"][row[0]]), 1]\n",
    "            \n",
    "        if df[\"p-adj\"][row[0]] < (alpha):\n",
    "                cld.iat[groups.index(df[\"group1\"][row[0]]), 3] +=  cld.iat[groups.index(df[\"group2\"][row[0]]), 1]\n",
    "                cld.iat[groups.index(df[\"group2\"][row[0]]), 3] +=  cld.iat[groups.index(df[\"group1\"][row[0]]), 1]\n",
    "\n",
    "    cld[2] = cld[2].apply(lambda x: \"\".join(sorted(x)))\n",
    "    cld[3] = cld[3].apply(lambda x: \"\".join(sorted(x)))\n",
    "    cld.rename(columns={0: \"groups\"}, inplace=True)\n",
    "\n",
    "    # this part will reassign the final name to the group\n",
    "    # for sure there are more elegant ways of doing this\n",
    "    cld[\"labels\"] = \"\"\n",
    "    letters = list(string.ascii_lowercase)\n",
    "    unique = []\n",
    "    for item in cld[2]:\n",
    "\n",
    "        for fitem in cld[\"labels\"].unique():\n",
    "            for c in range(0, len(fitem)):\n",
    "                if not set(unique).issuperset(set(fitem[c])):\n",
    "                    unique.append(fitem[c])\n",
    "        g = len(unique)\n",
    "\n",
    "        for kitem in cld[1]:\n",
    "            if kitem in item:\n",
    "                #Checking if there are forbidden pairing (proposition of solution to the imperfect script)                \n",
    "                forbidden = set()\n",
    "                for row in cld.itertuples():\n",
    "                    if letters[g] in row[5]:\n",
    "                        forbidden |= set(row[4])\n",
    "                if kitem in forbidden:\n",
    "                    g=len(unique)+1\n",
    "               \n",
    "                if cld[\"labels\"].loc[cld[1] == kitem].iloc[0] == \"\":\n",
    "                   cld[\"labels\"].loc[cld[1] == kitem] += letters[g] \n",
    "               \n",
    "                # Checking if columns 1 & 2 of cld share at least 1 letter\n",
    "                if len(set(cld[\"labels\"].loc[cld[1] == kitem].iloc[0]).intersection(cld.loc[cld[2] == item, \"labels\"].iloc[0])) <= 0:\n",
    "                    if letters[g] not in list(cld[\"labels\"].loc[cld[1] == kitem].iloc[0]):\n",
    "                        cld[\"labels\"].loc[cld[1] == kitem] += letters[g]\n",
    "                    if letters[g] not in list(cld[\"labels\"].loc[cld[2] == item].iloc[0]):\n",
    "                        cld[\"labels\"].loc[cld[2] == item] += letters[g]\n",
    "\n",
    "    cld = cld.sort_values(\"labels\")\n",
    "    cld.drop(columns=[1, 2, 3], inplace=True)\n",
    "    print(cld)\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    return(cld)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "008563563e781916d6359c24295d5508987d9d4a8f497f916ed83e23a440f2f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
